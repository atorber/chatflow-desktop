apiVersion: v1
data:
  launch.sh: |-
    #! /bin/bash
    
    MEGATRON_PATH=${MEGATRON_PATH:-"/workspace/AIAK-Megatron"}
    AIAK_TRAINING_PATH=${AIAK_TRAINING_PATH:-"/workspace/AIAK-Training-LLM"}
    
    DATA_PATH=${DATA_PATH:-"/mnt/cluster/aiak-training-llm/dataset/sft_aplaca_zh_data.json"}
    
    #DATA_PATH=${DATA_PATH:-"/mnt/cluster/aiak-training-llm/baichuan2/sft_aplaca_zh_tokenized"}
    
    DATASET_CONFIG_PATH=${DATASET_CONFIG_PATH:-"/workspace/AIAK-Training-LLM/configs/sft_dataset_config.json"}
    
    DATA_CACHE_PATH=${DATA_CACHE_PATH:-"/mnt/cluster/aiak-training-llm/baichuan2/sft_aplaca_zh_data_cache"}
    
    TOKENIZER_PATH=${TOKENIZER_PATH:-"/mnt/cluster/huggingface.co/baichuan-inc/Baichuan2-13B-Base/"}
    
    CHECKPOINT_PATH=${CHECKPOINT_PATH:-"/mnt/cluster/aiak-training-llm/baichuan2/baichuan2_13b_tp1_pp2_fp16"}
    
    TENSORBOARD_PATH=${TENSORBOARD_PATH:-"/mnt/cluster/aiak-training-llm/tensorboard-log/baichuan2-13B-sft"}
    
    GPUS_PER_NODE=8
    
    # Change for multinode config
    MASTER_ADDR=${MASTER_ADDR:-"localhost"}
    MASTER_PORT=${MASTER_PORT:-"6000"}
    NNODES=${WORLD_SIZE:-"1"}
    NODE_RANK=${RANK:-"0"}
    
    DISTRIBUTED_ARGS=(
        --nproc_per_node $GPUS_PER_NODE
        --nnodes $NNODES
        --node_rank $NODE_RANK
        --master_addr $MASTER_ADDR
        --master_port $MASTER_PORT
    )
    
    # you can setup baichuan2-13b maunally
    #MODEL_ARGS=(
    #    --model-name baichuan2
    #    --num-layers 40
    #    --hidden-size 5120
    #    --ffn-hidden-size 13696
    #    --num-attention-heads 40
    #    --position-embedding-type alibi
    #    --normalization RMSNorm
    #    --swiglu
    #    --attention-dropout 0
    #    --hidden-dropout 0
    #    --disable-bias-linear
    #    --no-position-embedding
    #    --untie-embeddings-and-output-weights
    #)
    
    # or you can setup baichuan2-13b by using the following command
    MODEL_ARGS=(
        --model-name baichuan2-13b # options: baichuan2-7b, baichuan2-13b
    )
    
    DATA_ARGS=(
        --tokenizer-type HFTokenizer
        --hf-tokenizer-path $TOKENIZER_PATH
        --data-path $DATA_PATH
        --split 100,0,0
    )
    
    SFT_ARGS=(
        --chat-template baichuan2
        --variable-seq-lengths
        --padding-side right
        --sft-num-preprocess-workers 16
        --no-check-for-nan-in-loss-and-grad
        # --is-tokenized-data
    
        #--sft-dataset-config $DATASET_CONFIG_PATH
        #--sft-dataset sft_aplaca_zh_data # defined in --sft-dataset-config, default: default
        #--data-cache-path $DATA_CACHE_PATH
    
        #--sft-data-streaming
        #--train-on-prompt
        #--eod-mask-loss
    )
    TRAINING_ARGS=(
        --training-phase sft
        --seq-length 4096
        --max-position-embeddings 4096
        --init-method-std 0.01
        --micro-batch-size 1
        --global-batch-size 128
        --lr 0.0002
        --min-lr 1.0e-5
        --clip-grad 1.0
        --weight-decay 0.01
        --optimizer adam
        --adam-beta1 0.9
        --adam-beta2 0.95
        --adam-eps 1e-05
        --norm-epsilon 1e-6
        --train-iters 5000
        --lr-decay-iters 5000
        --lr-decay-style cosine
        --lr-warmup-fraction 0.002
        --initial-loss-scale 65536
        --fp16
        --load $CHECKPOINT_PATH
        --save $CHECKPOINT_PATH
        --save-interval 500
        --eval-interval 100
        --eval-iters 10
        #--no-load-optim
        #--no-load-rng
    )
    
    MODEL_PARALLEL_ARGS=(
        --tensor-model-parallel-size 1
        --pipeline-model-parallel-size 2
        --use-distributed-optimizer
        --overlap-grad-reduce
        --overlap-param-gather
        --sequence-parallel
        --distributed-backend nccl
    )
    
    LOGGING_ARGS=(
        --log-interval 1
        --tensorboard-dir ${TENSORBOARD_PATH}
        --log-timers-to-tensorboard
    )
    
    if [ -n "${WANDB_API_KEY}" ]; then
        LOGGING_ARGS+=(
            --wandb-project ${WANDB_PROJECT}
            --wandb-exp-name ${WANDB_NAME}
        )
    fi
    
    PYTHONPATH=$MEGATRON_PATH:$AIAK_TRAINING_PATH:$PYTHONPATH \
        torchrun ${DISTRIBUTED_ARGS[@]} \
        $AIAK_TRAINING_PATH/aiak_training_llm/train.py \
        ${MODEL_ARGS[@]} \
        ${DATA_ARGS[@]} \
        ${TRAINING_ARGS[@]} \
        ${SFT_ARGS[@]} \
        ${MODEL_PARALLEL_ARGS[@]} \
        ${LOGGING_ARGS[@]}


kind: ConfigMap
metadata:
  name: launch-cm-llama2-7b-tp1-pp1-dp8-zero1
  namespace: default
---
apiVersion: "kubeflow.org/v1"
kind: PyTorchJob
metadata:
  name: megatron-llama2-7b-tp1-pp1-dp8-zero1
  namespace: default
spec:
  pytorchReplicaSpecs:
    Master:
      replicas: 1
      restartPolicy: Never
      template:
        metadata:
        spec:
          schedulerName: volcano
          containers:
            - name: pytorch
              image: registry.baidubce.com/aihc-aiak/aiak-training-llm:ubuntu22.04-cu12.3-torch2.2.0-py310_v2.1.0.1_release
              imagePullPolicy: Always
              command:
                - bash
                #- -c
                #- sleep 1d
                - /workspace/Megatron-LM/examples/launch.sh
              env:
                - name: CUDA_DEVICE_MAX_CONNECTIONS
                  value: "1"
                       
              resources:
                limits:
                  nvidia.com/gpu: 8
                  rdma/hca: 1
              securityContext:
                capabilities:
                  add: [ "IPC_LOCK" ]
              volumeMounts:
                - mountPath: /dev/shm
                  name: cache-volume
                - name: config-volume
                  mountPath: /workspace/Megatron-LM/examples/launch.sh
                  subPath: launch.sh
                - name: data
                  mountPath: /mnt/cluster
          volumes:
            - name: cache-volume
              emptyDir:
                medium: Memory
            - name: config-volume
              configMap:
                name: launch-cm-llama2-7b-tp1-pp1-dp8-zero1
            - name: data
              persistentVolumeClaim:
                claimName: pvc-pfs
